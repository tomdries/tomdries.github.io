---
title: "Using ChatGPT to Create Virtual Research Participants"
description: "An Exploration of AI-Generated Personas in Personality Research"
pubDate: "August 1 2024"
heroImage: "/images/gpt-personas.png"
tags: ["research paper"]
---

* The <a href="https://doi.org/10.1016/j.paid.2024.112729" target="_blank">full research paper</a> was recently published in Personality and Individual Differences. Below is a summary.

*Can ChatGPT simulate human respondents? In this study, we let GPT-4 create personas and reply to personality research questionnaires. We observe human-like patterns, but also observe differences. We conclude that LLMs like ChatGPT hold promise for domains reaching from questionnaire research to product design, though it certainly raises interesting further questions.*

Traditional personality research heavily relies on questionnaires, which come with inherent limitations. Respondents might give socially desirable answers, get fatigued, or simply not have time to participate. This got us thinking: Could large language models like ChatGPT help in preliminary personality research?

In our study, we had ChatGPT generate 2,000 text-based personas - fictional characters with specific ages, genders, and brief personality descriptions. For each persona, we had ChatGPT complete several well-known personality questionnaires, including a short version of the Big Five Inventory (BFI-10) and scales measuring sensation-seeking and dark personality traits.

The results were intriguing. When we analyzed how ChatGPT scored these personas on the Big Five personality dimensions (Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism), we found clear patterns that aligned with human psychology research. The average scores for different questionnaire items correlated strongly (r = 0.93) with previously published human data.

However, we also noticed some interesting differences. ChatGPT's personas showed more extreme scores than human respondents typically do. Some correlations between personality traits also differed from what we see in human studies. For example, while humans typically show a positive correlation between extraversion and conscientiousness, ChatGPT's personas showed a negative correlation.

We then explored whether changing how we asked ChatGPT to create personas would affect the results. We tried generating more realistic personas, personas with explicit personality descriptions, and even cinematic characters. Each approach yielded slightly different patterns in the personality traits, highlighting how the input prompts influence the outcomes.

What does this mean for the future of personality research? While our method isn't ready to replace traditional research with human participants, it shows promise for several applications:

- Testing questionnaires before using them with real participants
- Exploring how different types of questions might affect responses
- Training and education in personality research
- Developing more diverse personas for product design
- Creating more realistic characters for games and simulations

The study cost about $1,500 in API credits and took several days to complete, so there's room for improvement in efficiency. However, as language models continue to advance we expect them to rapidly become cheaper, and these types of applications could become increasingly practical.

This research represents an early step in exploring how AI can complement traditional personality research methods. While it perhaps raises more questions than it answers, it opens up exciting possibilities for future research at the intersection of artificial intelligence and psychology.

<a href="https://doi.org/10.1016/j.paid.2024.112729" target="_blank">Link to the full paper</a>